{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imaplib\n",
    "import email\n",
    "from email.header import decode_header\n",
    "import webbrowser\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "mail_server=\"imap.qiye.163.com\"\n",
    "mail_account=\"213201447@seu.edu.cn\" \n",
    "mail_password=\"15598444081ABC\"\n",
    "\n",
    "def connect_to_email(server, account, password):\n",
    "    mail = imaplib.IMAP4_SSL(server)\n",
    "    mail.login(account, password)\n",
    "    return mail\n",
    "\n",
    "\n",
    "def fetch_emails(mail, folder=\"INBOX\"):\n",
    "    mail.select(folder)\n",
    "\n",
    "    status, messages = mail.search(None, \"ALL\")\n",
    "    email_ids = messages[0].split()\n",
    "    return email_ids\n",
    "\n",
    "\n",
    "def read_email(mail, email_id):\n",
    "    res, msg = mail.fetch(email_id, \"(RFC822)\")\n",
    "    paper_titles = []\n",
    "    for response in msg:\n",
    "        if isinstance(response, tuple):\n",
    "            # 解析邮件\n",
    "            msg = email.message_from_bytes(response[1])\n",
    "            \n",
    "            # 获取邮件主题\n",
    "            subject, encoding = decode_header(msg[\"Subject\"])[0]\n",
    "            if isinstance(subject, bytes):\n",
    "                # 如果是字节，则根据编码解码\n",
    "                subject = subject.decode(encoding if encoding else \"utf-8\")\n",
    "            \n",
    "            # 获取发件人信息\n",
    "            from_ = msg.get(\"From\")\n",
    "            if \"<scholaralerts-noreply@google.com>\" in from_:\n",
    "                print(f\"主题: {subject}\")\n",
    "                print(f\"发件人: {from_}\")\n",
    "                # 检查邮件内容是否是多部分的\n",
    "                content_type = msg.get_content_type()\n",
    "                body = msg.get_payload(decode=True).decode()\n",
    "                \n",
    "                soup = BeautifulSoup(body, 'html.parser')\n",
    "                papers = []\n",
    "                \n",
    "                \"\"\"\n",
    "                papers = []\n",
    "                pattern = r'line-height:22px\">(.*?)</a></h3>'\n",
    "                pdf_links_1 = re.findall(r'url=([^&\"]+\\.pdf)', body)\n",
    "                pdf_links_2 = re.findall(r'url=([^&\"]+/pdf/[^&\"]*)', body)\n",
    "                \n",
    "                for i in pdf_links_1 + pdf_links_2:\n",
    "                    paper_titles.append(i)\n",
    "                paper_titles = list(set(paper_titles))\n",
    "                break\n",
    "                \"\"\"\n",
    "                for h3 in soup.find_all('h3', {'style': 'font-weight:normal;margin:0;font-size:17px;line-height:20px;'}):\n",
    "                    paper = {}\n",
    "                    \n",
    "                    # 提取论文标题\n",
    "                    title_tag = h3.find('a', class_='gse_alrt_title')\n",
    "                    if title_tag:\n",
    "                        paper['title'] = title_tag.get_text()\n",
    "                        link = title_tag['href']\n",
    "                        pdf_link_1 = re.findall(r'url=([^&\"]+\\.pdf)', link)\n",
    "                        pdf_link_2 = re.findall(r'url=([^&\"]+/pdf/[^&\"]*)', link)\n",
    "                        \n",
    "                        # 如果找到符合正则的主链接，保留；否则置为空\n",
    "                        if pdf_link_1:\n",
    "                            paper['link'] = pdf_link_1[0]\n",
    "                        elif pdf_link_2:\n",
    "                            paper['link'] = pdf_link_2[0]\n",
    "                        else:\n",
    "                            paper['link'] = None\n",
    "                    \n",
    "                    # 提取作者和来源\n",
    "                    author_source_tag = h3.find_next_sibling('div', {'style': 'color:#006621;line-height:18px'})\n",
    "                    if author_source_tag:\n",
    "                        paper['author_source'] = author_source_tag.get_text()\n",
    "                    \n",
    "                    # 提取摘要\n",
    "                    abstract_tag = h3.find_next_sibling('div', class_='gse_alrt_sni')\n",
    "                    if abstract_tag:\n",
    "                        paper['abstract'] = abstract_tag.get_text(separator=' ', strip=True)\n",
    "                    \n",
    "                    # 提取保存、分享等链接（可选）\n",
    "                    links_tag = h3.find_next_sibling('div', {'style': 'width:auto'})\n",
    "                    if links_tag:\n",
    "                        share_links = []\n",
    "                        for a in links_tag.find_all('a', href=True):\n",
    "                            href = a['href']\n",
    "                            # 使用正则表达式筛选符合条件的PDF链接\n",
    "                            pdf_links_1 = re.findall(r'url=([^&\"]+\\.pdf)', href)\n",
    "                            pdf_links_2 = re.findall(r'url=([^&\"]+/pdf/[^&\"]*)', href)\n",
    "                            \n",
    "                            # 如果找到匹配的链接，则将其添加到 share_links 中\n",
    "                            if pdf_links_1:\n",
    "                                share_links.extend(pdf_links_1)\n",
    "                            if pdf_links_2:\n",
    "                                share_links.extend(pdf_links_2)\n",
    "                        \n",
    "                        paper['share_links'] = share_links\n",
    "                    papers.append(paper)\n",
    "            else:\n",
    "                pass\n",
    "    return papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主题: XU YANG - 新的相关研究工作\n",
      "发件人: =?UTF-8?B?R29vZ2xlIOWtpuacr+aQnOe0ouW/q+iurw==?= <scholaralerts-noreply@google.com>\n"
     ]
    }
   ],
   "source": [
    "server = mail_server\n",
    "account = mail_account\n",
    "password = mail_password\n",
    "mail = connect_to_email(server=server, account=account, password=password)\n",
    "email_ids = fetch_emails(mail)\n",
    "for email_id in email_ids[-2:]:\n",
    "    paper_titles = read_email(mail, email_id)\n",
    "    if paper_titles != []:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset',\n",
       "  'link': None,\n",
       "  'author_source': 'J Liu, S Chen, X He, L Guo, X Zhu, W Wang, J Tang\\xa0- IEEE Transactions on Pattern\\xa0…, 2024',\n",
       "  'abstract': 'In this paper, we propose the V ision-A udio-L anguage O mni-pe R ception pretraining model (VALOR) for multimodal understanding and generation. Unlike widely-studied vision-language pretraining models, VALOR jointly models the\\xa0…',\n",
       "  'share_links': []},\n",
       " {'title': 'Navigating the nuances: A fine-grained evaluation of vision-language navigation',\n",
       "  'link': 'https://arxiv.org/pdf/2409.17313',\n",
       "  'author_source': 'Z Wang, M Wu, Y Cao, Y Ma, M Chen, T Tuytelaars\\xa0- arXiv preprint arXiv:2409.17313, 2024',\n",
       "  'abstract': 'This study presents a novel evaluation framework for the Vision-Language Navigation (VLN) task. It aims to diagnose current models for various instruction categories at a finer-grained level. The framework is structured around the context\\xa0…',\n",
       "  'share_links': ['https://arxiv.org/pdf/2409.17313',\n",
       "   'https://arxiv.org/pdf/2409.17313',\n",
       "   'https://arxiv.org/pdf/2409.17313']},\n",
       " {'title': 'Visual Question Decomposition on Multimodal Large Language Models',\n",
       "  'link': 'https://arxiv.org/pdf/2409.19339',\n",
       "  'author_source': 'H Zhang, J Liu, Z Han, S Chen, B He, V Tresp, Z Xu…\\xa0- arXiv preprint arXiv\\xa0…, 2024',\n",
       "  'abstract': 'Question decomposition has emerged as an effective strategy for prompting Large Language Models (LLMs) to answer complex questions. However, while existing methods primarily focus on unimodal language models, the question decomposition\\xa0…',\n",
       "  'share_links': ['https://arxiv.org/pdf/2409.19339',\n",
       "   'https://arxiv.org/pdf/2409.19339',\n",
       "   'https://arxiv.org/pdf/2409.19339']},\n",
       " {'title': 'Prompting Video-Language Foundation Models with Domain-specific Fine-grained Heuristics for Video Question Answering',\n",
       "  'link': 'https://arxiv.org/pdf/2410.09380',\n",
       "  'author_source': 'T Yu, K Fu, S Wang, Q Huang, J Yu\\xa0- IEEE Transactions on Circuits and Systems for\\xa0…, 2024',\n",
       "  'abstract': 'Video Question Answering (VideoQA) represents a crucial intersection between video understanding and language processing, requiring both discriminative unimodal comprehension and sophisticated cross-modal interaction for accurate\\xa0…',\n",
       "  'share_links': ['https://arxiv.org/pdf/2410.09380',\n",
       "   'https://arxiv.org/pdf/2410.09380',\n",
       "   'https://arxiv.org/pdf/2410.09380']},\n",
       " {'title': 'Efficient Visual Question Answering on Embedded Devices: Cross-Modality Attention With Evolutionary Quantization',\n",
       "  'link': None,\n",
       "  'author_source': 'A Mishra, A Agarwala, U Tiwari, VN Rajendiran…\\xa0- 2024 IEEE International\\xa0…, 2024',\n",
       "  'abstract': 'Visual Question Answering (VQA) lies at the intersection of vision and language domains necessitating learning representations from multiple modalities. While the model development for VQA has witnessed tremendous growth, the efforts for its\\xa0…',\n",
       "  'share_links': []},\n",
       " {'title': 'Loong: Generating minute-level long videos with autoregressive language models',\n",
       "  'link': 'https://arxiv.org/pdf/2410.02757',\n",
       "  'author_source': 'Y Wang, T Xiong, D Zhou, Z Lin, Y Zhao, B Kang…\\xa0- arXiv preprint arXiv\\xa0…, 2024',\n",
       "  'abstract': 'It is desirable but challenging to generate content-rich long videos in the scale of minutes. Autoregressive large language models (LLMs) have achieved great success in generating coherent and long sequences of tokens in the domain of\\xa0…',\n",
       "  'share_links': ['https://arxiv.org/pdf/2410.02757',\n",
       "   'https://arxiv.org/pdf/2410.02757',\n",
       "   'https://arxiv.org/pdf/2410.02757']},\n",
       " {'title': 'FineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension',\n",
       "  'link': 'https://arxiv.org/pdf/2409.14750',\n",
       "  'author_source': 'J Liu, X Yang, W Li, P Wang\\xa0- arXiv preprint arXiv:2409.14750, 2024',\n",
       "  'abstract': 'Referring Expression Comprehension (REC) is a crucial cross-modal task that objectively evaluates the capabilities of language understanding, image comprehension, and language-to-image grounding. Consequently, it serves as an\\xa0…',\n",
       "  'share_links': ['https://arxiv.org/pdf/2409.14750',\n",
       "   'https://arxiv.org/pdf/2409.14750',\n",
       "   'https://arxiv.org/pdf/2409.14750']},\n",
       " {'title': 'AlignedCoT: Prompting Large Language Models via Native-Speaking Demonstrations',\n",
       "  'link': 'https://eleanor-h.github.io/publication/2024-yang-alignedcot/2024-yang-alignedcot.pdf',\n",
       "  'author_source': 'Z Yang, Y Huang, J Xiong, L Feng, X Liang, Y Wang…',\n",
       "  'abstract': 'Large Language Models prompting, such as using in-context demonstrations, is a mainstream technique for invoking LLMs to perform highperformance and solid complex reasoning (eg, mathematical reasoning, commonsense reasoning), and has\\xa0…',\n",
       "  'share_links': ['https://eleanor-h.github.io/publication/2024-yang-alignedcot/2024-yang-alignedcot.pdf',\n",
       "   'https://eleanor-h.github.io/publication/2024-yang-alignedcot/2024-yang-alignedcot.pdf',\n",
       "   'https://eleanor-h.github.io/publication/2024-yang-alignedcot/2024-yang-alignedcot.pdf']},\n",
       " {'title': 'Leveraging Coarse-to-Fine Grained Representations in Contrastive Learning for Differential Medical Visual Question Answering',\n",
       "  'link': 'https://papers.miccai.org/miccai-2024/paper/1957_paper.pdf',\n",
       "  'author_source': 'X Liang, Y Wang, D Wang, Z Jiao, H Zhong, M Yang…\\xa0- International Conference on\\xa0…, 2024',\n",
       "  'abstract': 'Abstract Chest X-ray Differential Medical Visual Question Answering (Diff-MedVQA) is a novel multi-modal task designed to answer questions about diseases, especially their differences, based on a main image and a reference image. Compared to the\\xa0…',\n",
       "  'share_links': ['https://papers.miccai.org/miccai-2024/paper/1957_paper.pdf',\n",
       "   'https://papers.miccai.org/miccai-2024/paper/1957_paper.pdf',\n",
       "   'https://papers.miccai.org/miccai-2024/paper/1957_paper.pdf']},\n",
       " {'title': 'Supplementary Materials SAM4MLLM: Enhance Multi-Modal Large Language Model for Referring Expression Segmentation',\n",
       "  'link': 'https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/10570-supp.pdf',\n",
       "  'author_source': 'YC Chen, WH Li, C Sun, YCF Wang, CS Chen',\n",
       "  'abstract': 'In Tab. 1, we provide an analysis for using SAM as our backend, where the upper- bound is the maximum IoU from multiple SAM prompts sampled using the ground- truth masks. The upper-bound is around 87.8% IoU, which is much higher than all\\xa0…',\n",
       "  'share_links': ['https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/10570-supp.pdf',\n",
       "   'https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/10570-supp.pdf',\n",
       "   'https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/10570-supp.pdf']}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_titles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
